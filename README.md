# Chanakya's Counsel

A Retrieval-Augmented Generation (RAG) chatbot that enables users to query the Arthashastra and receive responses delivered in the voice of Chanakya — ancient Indian philosopher, statesman, and author of the Arthashastra.

![Python](https://img.shields.io/badge/Python-3.10+-blue?style=flat-square)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green?style=flat-square)
![LangChain](https://img.shields.io/badge/LangChain-latest-orange?style=flat-square)
![Gemini](https://img.shields.io/badge/Gemini-2.5%20Flash-purple?style=flat-square)

<img width="1494" height="867" alt="Screenshot 2026-02-25 131559" src="https://github.com/user-attachments/assets/7474efac-9de0-4084-adbc-146400ecf358" />

---

## Introduction

Chanakya, also known as Kautilya or Vishnugupta, was a 4th-century BCE Indian philosopher, economist, jurist, and royal advisor who served as the principal architect behind the rise of the Mauryan Empire under Emperor Chandragupta Maurya. Born in Takshashila — one of the ancient world's greatest centers of learning — he is widely regarded as one of the most brilliant strategic minds in human history, often compared to Machiavelli and Sun Tzu. 

His magnum opus, the Arthashastra, is a comprehensive treatise spanning 15 books and over 6,000 sutras, covering statecraft, economic policy, military strategy, law, diplomacy, and the duties of a king. Written over two millennia ago, it remains one of the most sophisticated works of political philosophy ever produced, remarkable for its ruthless pragmatism, its attention to detail, and its surprisingly modern understanding of governance and human nature. Chanakya's legacy endures not merely as a historical figure but as a timeless authority on power, strategy, and the art of statecraft.

---

## Overview

This is a fullstack RAG and LLM-supported system that grounds every response in the actual text of the Arthashastra. Upon receiving a user query, the system retrieves the most semantically relevant passages from a vector database and injects them into a structured prompt before invoking the language model. This ensures that all responses are contextually accurate and traceable to the source material, rather than relying on the model's general training knowledge.

---

## Technology Stack

| Layer | Technology |
|---|---|
| Backend Framework | FastAPI |
| Large Language Model | Google Gemini 2.5 Flash |
| Embeddings | HuggingFace `BAAI/bge-small-en-v1.5` |
| Vector Database | ChromaDB |
| Orchestration | LangChain |
| Frontend | HTML, CSS, JavaScript, Tailwind CSS |

---

## Architecture

The application follows a standard RAG pipeline:

1. **Ingestion** — `ingest.py` loads the Arthashastra, splits it into overlapping text chunks, generates vector embeddings for each chunk using a HuggingFace sentence-transformer model, and persists the resulting embeddings to a local ChromaDB instance.
2. **Retrieval** — Upon receiving a user query, the backend embeds the query using the same model and performs a similarity search against the vector database, returning the top 12 most relevant passages.
3. **Generation** — The retrieved passages are injected into a structured prompt alongside the user's question. Google Gemini 2.5 Flash generates a response grounded strictly in the provided context, delivered in Chanakya's first-person voice.

---

## Project Structure

```
chanakya-counsel/
├── backend.py          # FastAPI server, RAG pipeline, Gemini integration
├── ingest.py           # One-time ingestion script to construct the vector database
├── index.html          # Frontend user interface
├── arthashastra.txt    # Source text of the Arthashastra
├── chroma_db/          # Persisted vector database (generated by ingest.py)
├── requirements.txt    # Python dependencies
└── .env                # Environment variables — API keys (not committed)
```

---

## Installation and Setup

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/chanakya-counsel.git
cd chanakya-counsel
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure Environment Variables

Create a `.env` file in the root directory with the following contents:

```
GEMINI_API_KEY=your_gemini_api_key_here
```

A free Gemini API key may be obtained at [aistudio.google.com](https://aistudio.google.com).

### 4. Build the Vector Database

This step is required only once. It processes the Arthashastra and populates the ChromaDB instance:

```bash
python ingest.py
```

Upon completion, a `chroma_db/` directory will be created in the project root.

### 5. Start the Application

```bash
uvicorn backend:app --reload
```

Navigate to `http://127.0.0.1:8000` in your browser to access the application.

---

## Notes on Embedding Model Selection

This project deliberately uses the open-source `BAAI/bge-small-en-v1.5` model from HuggingFace in place of commercial embedding APIs. The model runs entirely on local hardware, incurs no API costs, and delivers strong performance on semantic similarity tasks. The same model is used for both ingestion and retrieval, which is a requirement for vector similarity search to function correctly.
